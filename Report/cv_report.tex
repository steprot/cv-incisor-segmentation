\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}


% Use wide margins, but not quite so wide as fullpage.sty
\marginparwidth 0.5in 
\oddsidemargin 0.25in 
\evensidemargin 0.25in 
\marginparsep 0.25in
\topmargin 0.25in 
\textwidth 6in \textheight 8 in


\author{Orsolya Lukacs-Kisbandi, r0650042 \\
		Stefano Proto, r0650809}
\title{ \textbf{Computer Vision Report \\ 
		Incisor Segmentation} }
\date{\today}

\begin{document}

\maketitle
\newpage
\setcounter{page}{1}
\tableofcontents


\section{Introduction}

The aim of this project was to write  an  algorithm  that  is  capable  of  segmenting  the upper and lower incisors in panoramic radiographs  using  a  model  based  approach. The choosen method is called Active Shape Models and it is described in \cite{cootes1} Cootes at al. The implemented model can be broken down into sub-parts, namely: building the model, preprocessing the input images and fitting the model on the new image. In addition our observations and experimental results will be discussed in the final part of the report. 

\section{Active Shape Model}

The reason we are building an Active Shape Model is to create a more general representation of each incisor, which is not very sensitive to variability, this way allowing to analyse and process complex images such as radiographs. This method is based on a prior model of what is expected to be seen on the new image. In our case these are the incisors, represented with landmarks. Based on these landmarks, a general representation of the expected shape is created, which can be used to find the best match of the model in the new image. This model gives a compact representation of the previous knowledge allowing some variation, but being specific enough to not to allow changes which haven't been seen in the training data. The landmarks are points on the training images which represent a distinguishable point which appears on every training example.

\subsection{Landmarks} % Maybe rename this
The training set contains samples from several people, for each person every incisor is represented by a set of 40 landmark points describing the shape of the tooth. In order to create the models, the mean shape for each tooth is found. Then the shapes of the teeth had to be aligned, scaled and centred around the same mean point. To perform these operations, an iterative approach proposed in \cite{cootes1} has been implemented. By doing Generalized Procrustes Analysis iteratively until our model converged to a certain mean, we achieved a general representation of the incisor. The model was considered stable (it converged) when the squared sum of the distance of each shape to the mean was less than $10^{-10}$.

\subsection{Principle Component Analysis}

Principal Component Analysis is a statistical procedure used to identifying a smaller number of uncorrelated variables, called "principal components", from a large set of data to explain the maximum amount of variance with the fewest number of principal components. In other words, the aim of the PCA is to reduce the dimensionality of the data. This is an improvement both in terms of memory and computational power.\\
In our case, we set the number of principal components to 8, which covers 99\% of the variance of the data. This way, we reduced the number of landmarks from 40 to 4.

\section{Pre-processing}
Radiograph images are usually and intrinsically noisy and low in contrast. These characteristics make the images features difficult to be identified; in order to overcome this issue, sharpening techniques and contrast enhancement alogorithms are applied to the images before them being analyzed. \\
In order to reduce the noise different filters have been tried, having though the disadvantage of removing high-frequency features, and then introducing blurring edges; a trade-off among noise suppression, image deblurring and edge detection has then to be found. To achieve this goal an adaptive median filter and a bilateral filters are applied in order to reduce the impulsive and the Gaussian noise, without affecting the edges. Only afterwards the contrast of the image is enhanced by means of two gray-level morphology techniques (top-hat and bottom-hat transform) and an histogram equalization technique. \\
% since the adaptive median filter is really expensive, then we just applied a median filter with kernel sieze = 3, in order to not affect the edges. We saw experimentally that there is not that much difference in the outcome. The goal of the median filtering (medianBlur) is also to remove the salt and pepper noise, that anyway is not that much present in the radiographs. 
The Adaptive Median filtering removes the salt-and-pepper noise, also called impulsive noise. However, it can't remove noise that spreads large areas of image because it would also destroy other features like edges of teeth. The bilateral filter is therefore applied. It replaces the intensity of each pixel with the weighted average of intensity values from nearby pixels, to equalize the colours throughout the large areas and remove the noise without affecting the edges. \\
The next step is applying the top-hat transform, that is able to enhance brighter structures, and then the bottom-hat transform, that enhances the darker structures. \\ 
The last step is enhance the constrast of the image, and since several and different are the techniques available, analysis of the different possibilities have been made by many researchers. Among all the candidate algorithms, Ahmad et al. shown that the Contrast Limited Adaptive Histogram Equalization (CLAHE) better enhances the image quality and increases the diagnostic ease, overperforming both the Sharp Contrast Limited Adaptive Histogram Equalization (SCLAHE) and the Median Adaptive Histogram Equalization (SMAHE) \cite{ahmad}. This technique is based on the Adaptive Histogram Equalization (AHE): it divides the image in small blocks, \textit{tiles}, and then equalize their histograms. This allows the algorithm to adapt to local variations in an image like shadows and highlights that would not be treated by global histogram equalization. AHE could though amplify the noise, so the Contrast Limited technique is used: if any histogram bin is above the specified contrast limit, those pixels are clipped and distributed uniformly to other bins before applying histogram equalization.

\section{Fit the model to an image}
For each image a manual initiation is asked. The user has to draw boxes around the uppper and lower jaws: this allows the to simplify the fitting of the models of each tooth into the image. 

\begin{thebibliography}{99}

\bibitem{cootes1} Cootes, Tim, E. Baldock, and J. Graham. "An introduction to active shape models." \textit{Image processing and analysis} (2000): 223-248.

\bibitem{cootes2} Cootes, Timothy F., et al. "Active shape models-their training and application." \textit{Computer vision and image understanding} 61.1 (1995): 38-59.

\bibitem{blanz} Blanz, Volker, et al. "A statistical method for robust 3D surface reconstruction from sparse data." \textit{3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004. Proceedings. 2nd International Symposium on.} IEEE, 2004.

\bibitem{ahmad} Ahmad, Siti Arpah Bt, et al. "Utilizing contrast enhancement algorithms (CEAs) in identification of dental abnormalities." \textit{Technological Advances in Electrical, Electronics and Computer Engineering (TAEECE), 2013 International Conference on.} IEEE, 2013.

\bibitem{paper4}
Model reconstruction 

%\bibitem{cohnen} 
%M Cohnen, J Kemper, O M ̈obes, J Pawelzik, and U M ̈odder. Radiation dose in dental radiology.,
%\textit{European radiology}. 
%12(3):634–637, 2002
% 
%\bibitem{anil} 
%Anil K Jain and Hong Chen. Matching of dental x-ray images for human identification.,
%\\\textit{37(7):1519–1532, 2004.}

\end{thebibliography}

\end{document}