\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}


% Use wide margins, but not quite so wide as fullpage.sty
\marginparwidth 0.5in 
\oddsidemargin 0.25in 
\evensidemargin 0.25in 
\marginparsep 0.25in
\topmargin 0.25in 
\textwidth 6in \textheight 8 in


\author{Orsolya Lukacs-Kisbandi, r0650042 \\
		Stefano Proto, r0650809}
\title{ \textbf{Computer Vision Report \\ 
		Incisor Segmentation} }
\date{\today}

\begin{document}

\maketitle
\newpage
\setcounter{page}{1}
\tableofcontents


\section{Introduction}

The aim of this project was to write  an  algorithm  that  is  capable  of  segmenting  the upper and lower incisors in panoramic radiographs  using  a  model  based  approach. The choosen method is called Active Shape Models and it is described in \cite{cootes1} Cootes at al. The implemented model can be broken down into sub-parts, namely: building the model, preprocessing the input images and fitting the model on the new image. In addition our observations and experimental results will be discussed in the final part of the report. 

\section{Active Shape Model}

The reason we are building an Active Shape Model is to create a more general representation of each incisor, which is not very sensitive to variability, this way allowing to analyse and process complex images such as radiographs. This method is based on a prior model of what is expected to be seen on the new image. In our case these are the incisors, represented with landmarks. Based on these landmarks, a general representation of the expected shape is created, which can be used to find the best match of the model in the new image. This model gives a compact representation of the previous knowledge allowing some variation, but being specific enough to not to allow changes which haven't been seen in the training data. The landmarks are points on the training images which represent a distinguishable point which appears on every training example.

\subsection{Landmarks} % Maybe rename this
The training set contains samples from several people, for each person every incisor is represented by a set of 40 landmark points describing the shape of the tooth. In order to create the models, the mean shape for each tooth is found. Then the shapes of the teeth had to be aligned, scaled and centred around the same mean point. To perform these operations, an iterative approach proposed in \cite{cootes1} has been implemented. By doing Generalized Procrustes Analysis iteratively until our model converged to a certain mean, we achieved a general representation of the incisor. The model was considered stable (it converged) when the squared sum of the distance of each shape to the mean was less than $10^{-10}$.

\subsection{Principle Component Analysis}

Principal Component Analysis is a statistical procedure used to identifying a smaller number of uncorrelated variables, called "principal components", from a large set of data to explain the maximum amount of variance with the fewest number of principal components. In other words, the aim of the PCA is to reduce the dimensionality of the data. This is an improvement both in terms of memory and computational power.\\
In our case, we set the number of principal components to 8, which covers 99\% of the variance of the data. This way, we reduced the number of landmarks with a significant amount.

\section{Pre-processing}
Radiographs are usually noisy and low in contrast. These characteristics make the image features difficult to identify. In order to overcome this issue, sharpening techniques and contrast enhancement algorithms are applied to the images before they are analysed. \\
In order to reduce noise, we experimented with different filters, each having it's own advantages and disadvantages.  A balance between these filters had to be found, since some of them removed important image filter such as edges and high contrast points. For reducing the impulsive and the Gaussian noise, without affecting the edges a median filter and a bilateral filter is applied. The median filter considers each pixel in the image in turn and looks at the nearby pixels to decide whether or not it is representative of its surroundings. It replaces the pixel value with the median of the neighbouring pixel values. The median is calculated by first sorting all the pixel values from the surrounding neighbourhood into numerical order and then replacing the pixel being considered with the middle pixel value.  Instead of the simple median filter, we tried to apply an adaptive median filter but this required too much computational power as well as time, so we decided to use the much simpler version of this. The Bilateral filter  is a non-linear, edge-preserving and noise-reducing smoothing filter. The intensity value at each pixel in an image is replaced by a weighted average of intensity values from nearby pixels, which is based on a Gaussian distribution. Its formulation is simple: each pixel is replaced by an average of its neighbours. It depends only on two parameters that indicate the size and contrast of the features to preserve \cite{paris}.\\
After these transformations the contrast of the image is enhanced by means of two gray-level morphology techniques (top-hat and bottom-hat transform) and a histogram equalization technique.  \\

The Adaptive Median filtering removes the salt-and-pepper noise, also called impulsive noise. However, it can't remove noise that spreads large areas of image because it would also destroy other features like edges, the bilateral filter is therefore applied. It replaces the intensity of each pixel with the weighted average of intensity values from nearby pixels, to equalize the colours throughout the large areas and remove the noise without affecting the edges. \\
The next step is applying the top-hat transform, that is able to enhance brighter structures, and then the bottom-hat transform, that enhances the darker structures. \\ 
The final step is enhanceing the constrast of the image. Several different techniques are available, hence there had been a lot of research on it. Among all the candidate algorithms, Ahmad et al. shown that the Contrast Limited Adaptive Histogram Equalization (CLAHE) better enhances the image quality and increases the diagnostic ease, overperforming both the Sharp Contrast Limited Adaptive Histogram Equalization (SCLAHE) and the Median Adaptive Histogram Equalization (SMAHE) \cite{ahmad}. This technique is based on the Adaptive Histogram Equalization (AHE): it divides the image in small blocks, \textit{tiles}, and then equalizes the histograms. This allows the algorithm to adapt to local variations in an image such as shadows and highlights which would not be handled by global histogram equalization. AHE could amplify the noise, so the Contrast Limited technique is used: if any histogram bin is above a specified contrast limit, those pixels are clipped and distributed uniformly to other bins before applying histogram equalization.

\section{Fit the model to an image}
Fitting the models on the image consists of multiple stages. At first hand drawn boxes have to be created, than they are used in \textit{best\_box} function to fine tune them. After returning these boxes the active shape models are fitted on the image.

\subsection{Hand draw boxes}

\subsection{Fine-tune boxes}

\subsection{Fit Active shape models}

\begin{thebibliography}{99}

\bibitem{cootes1} Cootes, Tim, E. Baldock, and J. Graham. "An introduction to active shape models." \textit{Image processing and analysis} (2000): 223-248.

\bibitem{cootes2} Cootes, Timothy F., et al. "Active shape models-their training and application." \textit{Computer vision and image understanding} 61.1 (1995): 38-59.

\bibitem{blanz} Blanz, Volker, et al. "A statistical method for robust 3D surface reconstruction from sparse data." \textit{3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004. Proceedings. 2nd International Symposium on.} IEEE, 2004.

\bibitem{ahmad} Ahmad, Siti Arpah Bt, et al. "Utilizing contrast enhancement algorithms (CEAs) in identification of dental abnormalities." \textit{Technological Advances in Electrical, Electronics and Computer Engineering (TAEECE), 2013 International Conference on.} IEEE, 2013.

\bibitem{paris} Paris, S., Kornprobst, P., Tumblin, J. and Durand, F., 2007, August. \textit{A gentle introduction to bilateral filtering and its applications.}, In ACM SIGGRAPH 2007 courses (p. 1). ACM.

\bibitem{paper4}
Model reconstruction 

%\bibitem{cohnen} 
%M Cohnen, J Kemper, O M ̈obes, J Pawelzik, and U M ̈odder. Radiation dose in dental radiology.,
%\textit{European radiology}. 
%12(3):634–637, 2002
% 
%\bibitem{anil} 
%Anil K Jain and Hong Chen. Matching of dental x-ray images for human identification.,
%\\\textit{37(7):1519–1532, 2004.}

\end{thebibliography}

\end{document}